#### 一、大模型部署与应用介绍

***1、部署大模型必备知识*：**类OPENAI接口：RESTful风格api，大模型api接口

 			大模型相关库概念

 			windows：环境搭建

 			linux：autodl，wsl2

 				多轮对话大模型：基于fastapi，WEBUI（streanlit，gradio）

 			temperature和top\_p参数常见设置

#### 二、RESTful风格 API

***1、特点：***

&nbsp;	无状态性：每个请求都是独立的，服务器不存储客户的上下文信息

&nbsp;	统一接口：统一接口，就是为分布式系统设计了一套“通用协议”或“标准礼仪”统一的地址（URI）来定位资源，统一的动作（HTTP方法）来表达意图，统一的语言（JSON/XML等表述）来交换数据，统一的信号（HTTP状态码和头部）来沟通元数据，

&nbsp;	客户端—服务器架构：客户端与服务器职责分离，客户端负责用户界面，服务器负责存储和处理		

&nbsp;	资源的表述：通过不同的格式（如JSON，XML等）来表述资源状态

&nbsp;	可缓存性：客户端可以缓存服务器的响应，提高性能

***2、大模型中的RESTful API使用场景***

* 模型推理：客户端通过 POST 请求将输入(如文本、图像等)发送到大模型的 RESTful 接口，模型进行推理并返回结果。这些结果通常是以JSON 格式返回的结构化数据。
* 模型训练与更新：在一些场景中，RESTfuIAPI可以用于模型的微调。客户端上传新的训练数据并通过 API触发模型的重新训练或更新。
* 资源管理与监控：大规模模型的部署和运行涉及资源的调度和管理，如 GPU 分配模型负载均衡等。RESTfuIAPI可以用来查询当前模型的状态、资源使用情况、性能指标等。管理员通过 RESTfuI API 获取模型的健康状况，检查推理的延迟、吞吐量，甚至触发故障恢复。
* 任务队列与异步处理：在处理大模型的长时间任务时，RESTfuIAPI通过异步机制管理任务队列。客户端发送请求后，服务器通过 API 返回任务 ID，客户端可以在稍后通过 API 查询任务状态或获取结果。

***3、RESTfUIAPI在大模型中的使用优点***

* 易于集成和扩展:RESTfUIAPI具有标准化的接口，适用于各种前端系统、移动应用、其他服务模块进行交互。
* 轻量级:RESTfuIAPI通常基于 HTTP，数据传输采用JSON或XML，非常轻量易于实现跨平台、跨语言的通信。
* 无状态性和可伸缩性:无状态的设计便于扩展，系统可以轻松扩容，处理高并发请求，尤其适合大规模模型的云端部署。
* 良好的缓存支持:客户端和中间层(如CDN)可以缓存静态或重复查询的响应结果，减少负载，提升响应效率。

#### 三、使用api接口调用大模型

***1、***一共有三种形式：命令行工具，python的request库和openai库

***2、流程***

&nbsp;	大模型-----api接口------访问方式（cmd、python）

***3、在通过 API 调用大模型之前，通常需要从大模型服务提供商那里获取哪两个关键凭证信息?***

需要获取 API 密钥 (API Key)，用于身份认证;以及 基础 URL (Base URL)，即 API 服务的网络地址。

***4、在调用如 chat/completions 这样的聊天类大模型 API时，messages 参数扮演什么角色?它通常包含哪些内容?***

:messages 参数用于传递对话上下文历史。它是一个包含多个字典的列表，每个字典代表一条消息，并包含消息的角色(role:如 system, user, assistant)和内容(content )

***5、调用大模型 API 时，temperature 和 max tokens 这两个参数各自的主要作用是什么?***

temperature 主要控制模型生成内容的随机性或创造性(值越低越保守，越高越随机)。

&nbsp;max\_tokens 主要限制模型生成响应的最大长度(以token计数)。

***6、流式输出和非流式输出的区别，以及流式输出在哪些场景下具有优势？***

流式输出是模型一边生成一边输出文本内容，非流式输出是等待模型响应完成后一次性输出，它的优势在于可以更快地展示部分结果，提供更好的实时交互体验，尤其适合对话、代码生成等需要即时反馈的场景。





