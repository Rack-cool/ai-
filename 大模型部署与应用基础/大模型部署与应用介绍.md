#### 一、大模型部署与应用介绍

\*\**1、部署大模型必备知识*：类OPENAI接口：RESTful风格api，大模型api接口

 			大模型相关库概念

 			windows：环境搭建

 			linux：autodl，wsl2

 			多轮对话大模型：基于fastapi，WEBUI（streanlit，gradio）

 			temperature和top\_p参数常见设置

#### 二、RESTful风格 API

***1、特点：***

 	无状态性：每个请求都是独立的，服务器不存储客户的上下文信息

 	统一接口：统一接口，就是为分布式系统设计了一套“通用协议”或“标准礼仪”统一的地址（URI）来定位资源，统一的动作（HTTP方法）来表达意图，统一的语言（JSON/XML等表述）来交换数据，统一的信号（HTTP状态码和头部）来沟通元数据，

 	客户端—服务器架构：客户端与服务器职责分离，客户端负责用户界面，服务器负责存储和处理

 	资源的表述：通过不同的格式（如JSON，XML等）来表述资源状态

 	可缓存性：客户端可以缓存服务器的响应，提高性能

***2、大模型中的RESTful API使用场景***

* 模型推理：客户端通过 POST 请求将输入(如文本、图像等)发送到大模型的 RESTful 接口，模型进行推理并返回结果。这些结果通常是以JSON 格式返回的结构化数据。
* 模型训练与更新：在一些场景中，RESTfuIAPI可以用于模型的微调。客户端上传新的训练数据并通过 API触发模型的重新训练或更新。
* 资源管理与监控：大规模模型的部署和运行涉及资源的调度和管理，如 GPU 分配模型负载均衡等。RESTfuIAPI可以用来查询当前模型的状态、资源使用情况、性能指标等。管理员通过 RESTfuI API 获取模型的健康状况，检查推理的延迟、吞吐量，甚至触发故障恢复。
* 任务队列与异步处理：在处理大模型的长时间任务时，RESTfuIAPI通过异步机制管理任务队列。客户端发送请求后，服务器通过 API 返回任务 ID，客户端可以在稍后通过 API 查询任务状态或获取结果。

***3、RESTfUIAPI在大模型中的使用优点***

* 易于集成和扩展:RESTfUIAPI具有标准化的接口，适用于各种前端系统、移动应用、其他服务模块进行交互。
* 轻量级:RESTfuIAPI通常基于 HTTP，数据传输采用JSON或XML，非常轻量易于实现跨平台、跨语言的通信。
* 无状态性和可伸缩性:无状态的设计便于扩展，系统可以轻松扩容，处理高并发请求，尤其适合大规模模型的云端部署。
* 良好的缓存支持:客户端和中间层(如CDN)可以缓存静态或重复查询的响应结果，减少负载，提升响应效率。

#### 三、使用api接口调用大模型

\*\*\*1、\*\*\*一共有三种形式：命令行工具，python的request库和openai库

***2、流程***

 	大模型-----api接口------访问方式（cmd、python）

***3、在通过 API 调用大模型之前，通常需要从大模型服务提供商那里获取哪两个关键凭证信息?***

需要获取 API 密钥 (API Key)，用于身份认证;以及 基础 URL (Base URL)，即 API 服务的网络地址。

***4、在调用如 chat/completions 这样的聊天类大模型 API时，messages 参数扮演什么角色?它通常包含哪些内容?***

:messages 参数用于传递对话上下文历史。它是一个包含多个字典的列表，每个字典代表一条消息，并包含消息的角色(role:如 system, user, assistant)和内容(content )

***5、调用大模型 API 时，temperature 和 max tokens 这两个参数各自的主要作用是什么?***

temperature 主要控制模型生成内容的随机性或创造性(值越低越保守，越高越随机)。

 max\_tokens 主要限制模型生成响应的最大长度(以token计数)。

***6、流式输出和非流式输出的区别，以及流式输出在哪些场景下具有优势？***

流式输出是模型一边生成一边输出文本内容，非流式输出是等待模型响应完成后一次性输出，它的优势在于可以更快地展示部分结果，提供更好的实时交互体验，尤其适合对话、代码生成等需要即时反馈的场景。

#### 四、大模型相关库与概念

### （一）相关库

***1、Hugging Face Transformers***

&nbsp;	Hugging Face Transformers 是一个开源的 Python 库，支持多种预训练的自然语言处理模型，如 BERT、GPT、T5 等数百个用于不同任务的预训练模型。它可以轻松实现模型加载、微调、文本生成等功能。

链接:https://github.com/huggingface/transformers

***2、魔搭社区***

链接:https://community.modelscope.cn/.

&nbsp;	ModelScope旨在打造下一代开源的模型即服务共享平台，为泛AI开发者提供灵活易用、低成本的一站式模型服务产品，让模型应用更简单。

***3、OpenAI API***

&nbsp;	OpenAl 提供的 API 可以让开发者访问训练好的大语言模型，如 GPT-3 和 GPT-4，通过 API 可以进行文本生成、问答、总结等多种任务。

链接:https://openai.com/

***4、Langchain***

&nbsp;	LangChain 是一个框架，用于开发由大语言模型驱动的应用程序。"它可以帮助开发者构建更复杂的交互式模型应用，如对话代理、自动化工作流等

***5、SentenceTransformers***

&nbsp;	SentenceTransformers 是一个基于 transformers 库的 Python 框架，专门用于句子和文本嵌入的生成。它可以通过将句子或段落编码成高维向量，实现相似性搜索、聚类、分类等任务。其核心思想是通过预训练模型(如 BERT、ROBERTa、T5 等)来生成句子的向量表示，并支持多种语言。

***6、Deepspeed***

&nbsp;	DeepSpeed 是微软开源的一个深度学习优化库，专门为大规模模型的训练和推理进行优化，帮助开发者加速模型的训练过程，降低资源消耗。

### （二）模型的概念

***1、因果语言模型（causal language models）***

&nbsp;	因果语言模型(causal Language Models)，也被称为自回归语言模型(autoregressive language models)或仅解码器语言模型 (decoder-only languagemodels)，是一种机器学习模型，旨在根据序列中的前导 token 预测下一个 token换句话说，它使用之前生成的 token 作为上下文，一次生成一个 token 的文本。因果“方面指的是模型在预测下一个 token 时只考虑过去的上下文(即已生成的token)，而不考虑任何未来的 token 。

&nbsp;	因果语言模型被广泛用于涉及文本补全和生成的各种自然语言处理任务。它们在生成连贯且具有上下文关联性的文本方面尤其成功，这使得它们成为现代自然语言理解和生成系统的基础。



&nbsp;	在深度学习中，被称为语言模型的主要有三类:

* &nbsp;	序列到序列模型(sequence-to-sequence models):T5及其类似模型序列到序列模型同时使用编码器来捕获整个输入序列，以及解码器来生成输出序列。它们广泛应用于诸如机器翻译、文本摘要等任务。
* 双向模型 (bidirectional models)或仅编码器模型(encoder-only models):BERT及其类似模型

双向模型在训练期间可以访问序列中的过去和未来上下文。由于需要未来上下文，它们无法实时生成顺序输出。它们广泛用作嵌入模型，并随后用于文本分类。

* 因果语言模型(casuallanguage models)或仅解码器模型(decoder-onlymodels): GPT及其类似模型

因果语言模型以严格向前的单向方式运行，仅根据序列中的前导词汇预测每个后续词汇。这种单向性确保了模型的预测不依赖于未来上下文，使它们适合于文本补全和生成等任务。



***2、预训练(Pre-training)和基模型(Base models)***

&nbsp;	基础语言模型 (base language models)是在大量文本语料库上训练的基本模型，用于预测序列中的下一个词。它们的主要目标是捕捉语言的统计模式和结构，使它们能够生成连贯且具有上下文关联性的文本。这些模型具有多功能性，可以通过微调适应各种自然语言处理任务。虽然擅长生成流畅的文本，但它们可能需要情境学习(in-context learning)或额外训练才能遵循特定指令或有效执行复杂推理任务

* &nbsp;	对于 Qwen 模型，基础模型是指那些没有“-Instruct"标识符的模型，例如Qwen2.5-7B 和 Qwen2.5-72B
* &nbsp;	对于某些模型，基础模型是指那些没有“-Chat”标识符的模型。

&nbsp;	**要点：使用基础模型进行情境学习、下游微调等。**

***3、后训练(Post-training)和指令微调模型(Instruction-tuned models)***

&nbsp;	指令微调语言模型(Instruction-tuned language models)是专门设计用于理解并以对话风格执行特定指令的模型。

&nbsp;	这些模型经过微调，能准确地解释用户命令，并能以更高的准确性和一致性执行诸如摘要、翻译和问答等任务。与在大量文本语料库上训练的基础模型不同，指令调优模型会使用包含指令示例及其预期结果的数据集进行额外训练，通常涵盖多个回合。

&nbsp;	这种训练方式使它们非常适合需要特定功能的应用，同时保持生成流畅且连贯文本的能力。

对于 Qwen 模型，指令调优模型是指带有“-Instruct"后缀的模型，例如Owen2.5-7B-Instruct和Qwen2.5-72B-Instruct.

	**要点:使用指令微调模型进行对话式的任务执行、下游微调等。**

#### （三）token和tokenization

1、token代表的是模型处理和生成的基本单位，这些token可以是单词，子词和字符，具体取决于所采用的特定tokenzition方案

2、词表大小：模型识别的唯一token总数，也就是说模型可以识别多少token，词表就有多大

3、tokenization方案，按不同的标准分为四种方案：

&nbsp;	基于空格的tokenzation：将文本按空格分为很多个token

&nbsp;	基于字符的tokenzation：将每个字符视为一个单独的token

&nbsp;	基于子词的tokenzation：会将单词进一步分解为更小的部分，例如BPE（Byte Pair Encoding），wordpiece等技术就是基于这种思想设计的

&nbsp;	基于句子的tokenzation：将文本按句子来分割

&nbsp;	（1）BPE（Byte Pair Encoding）

* &nbsp;		基于字符的BPE：基本单位为字符，每个字符视为一个单独的token

&nbsp;		初始化词汇表:词汇表最初包含所有出现过的字符。合并操作:合并的是字符对。例如，对于字符串"hello”，初始词汇表可能包括\['h','e'，'1'，'o']，合并操作可能会将"he"合并为一个新的 token。

&nbsp;	适用场景:适用于处理标准的文本数据，特别是当文本数据主要由常见字符组成时。

&nbsp;		基于字节的BPE:每个字节（8位）被视为一个token



















